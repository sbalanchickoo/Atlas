{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Intro\n\nhttps://www.kaggle.com/c/learn-together\n\nMy attempt at the contest, upvoted kernels, which I found useful.\n\n**Resources**\n\n* [How to add table of contents](https://www.kaggle.com/questions-and-answers/69732)\n* [Hyperparameter Tuning](https://towardsdatascience.com/hyperparameter-tuning-c5619e7e6624)\n* [Stacking: Improve model performance](https://dkopczyk.quantee.co.uk/stacking/)"},{"metadata":{},"cell_type":"markdown","source":"# Versioning\n\n* Version: 4.0\n* Steps: \n  - Feature importance visualization\n  - MLXtend stacking, with layer one models below and RandomForest stacking model\n    - RandomForestClassifier\n    - XGBClassifier\n    - AdaBoostClassifier\n    - SVC\n    - KNeighborsClassifier\n  - Hyperparameter optimization for layer one using grid search\n  - For identifying stack model and validation, using X_train, y_train; For df_test fitting on X, Y"},{"metadata":{},"cell_type":"markdown","source":"# Table of contents <a id=\"0\"></a>\n* [Imports](#imports)\n* [Block selection](#block-selection)\n* [Define models](#define-models)\n* [Identify important Features](#identify-important-features)\n* [Feature engineering](#feature-engineering)\n* [Separate features and target](#separate-features-and-target)\n* [Get initial scores](#get-initial-scores)\n* [Grid search](#grid-search)\n* [Generate output](#generate-output)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\ndir = !ls -a\nif ('kernel-metadata.json' in dir):\n    src = 'Local'\n    # Local environment\n    data_path = './data/learn-together'\n    full_data = './data/covtype'\nelse:\n    # Kaggle environment\n    src = 'Kaggle'\n    data_path = '../input/learn-together'\n    full_data = '../input/covtype'\n\nprint('Environment set to [{env}]'.format(env=src))\nfor dirname, _, filenames in os.walk(data_path):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfor dirname, _, filenames in os.walk(full_data):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Suppress future defaults warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# System imports\nimport copy\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\n%matplotlib inline\n\n# Models\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\n# Utilities\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom mlxtend.classifier import StackingCVClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(data_path + '/test.csv')\ndf_sample_submission = pd.read_csv(data_path + '/sample_submission.csv')\ndf = pd.read_csv(data_path + '/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = list(df.columns)\nextra_cols = ['Cover_Type', 'Id']\nfor col in extra_cols:\n    if col in feature_cols:\n        feature_cols.remove(col)\ndf_full = pd.read_csv(full_data + '/covtype.csv')\ndf_in = pd.concat([df, df_test], axis=0)\ndf_full_new = pd.merge(df_full, df_in, how='inner', on=feature_cols, suffixes=('_x', '_y'), sort=True)\ndf_full_new.rename(columns={'Cover_Type_x': 'Cover_Type', 'Cover_Type_y': 'Existing'}, inplace=True)\ndf_full_new.loc[:, 'Existing'] = df_full_new['Existing'].notnull()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate features and target\ntarget = 'Cover_Type'\nfeatures = list(df.columns)\nfeatures.remove(target)\n\nX = df[features]\ny = df[target]\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, train_size=0.8)\nX_test = df_full_new.loc[df_full_new['Existing'] == False, features]\ny_test = df_full_new.loc[df_full_new['Existing'] == False, target]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Block selection <a id=\"block-selection\"></a>\n[Go back to top](#0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model_combinations = 0\nget_feature_importances = 0\ndrop_low_correlation_features = 0\ngrid_search = 0\nvalidation = 0\ngenerate_output = 1\nget_initial_scores = 0\n\nlow_correlation_features = []\ngrid_search_n_splits = 5\nlayer_one_folds = 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define models <a id=\"define-models\"></a>\n[Go back to top](#0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define Base (level 0) and Stacking (level 1) estimators\n# Commented rows were during Hyperparameter optimization run\nbase_models = []\n\nlgbm_clf = {}\nlgbm_clf['model'] = LGBMClassifier(num_leaves=128, verbose=-1, random_state=5, n_jobs=1)\nlgbm_clf['params'] = {'n_estimators': [100, 200, 300, 400]}\nlgbm_clf['grid_search'] = 0\nbase_models.append(lgbm_clf)\n\netc_clf = {}\netc_clf['model'] = ExtraTreesClassifier(n_estimators=300, min_samples_leaf=2, min_samples_split=2, max_depth=50\n                               , random_state=5, n_jobs=1)\netc_clf['params'] = {'n_estimators': [200, 300, 400, 500]}\netc_clf['grid_search'] = 0\nbase_models.append(etc_clf)\n\nadb_clf = {}\nadb_clf['model'] = AdaBoostClassifier(n_estimators=200, random_state=5)\nadb_clf['params'] = {'n_estimators': [100, 150, 200, 400]}\nadb_clf['grid_search'] = 0\nbase_models.append(adb_clf)\n\nsvc_clf = {}\nsvc_clf['model'] = SVC(probability=True, random_state=5, gamma='scale')\nsvc_clf['params'] = {'C': [0.01, 0.1, 1, 10, 100]}\nsvc_clf['grid_search'] = 0\nbase_models.append(svc_clf)\n\nxgb_clf = {}\nxgb_clf['model'] = XGBClassifier(random_state=5)\nxgb_clf['params'] = {}\nxgb_clf['grid_search'] = 0\nbase_models.append(xgb_clf)\n\nrf_clf = {}\nrf_clf['model'] = RandomForestClassifier(n_estimators=400, random_state = 5)\nrf_clf['params'] = {'n_estimators': [200, 300, 400]}\nrf_clf['grid_search'] = 0\nbase_models.append(rf_clf)\n\nknn_clf = {}\nknn_clf['model'] = KNeighborsClassifier()\nknn_clf['params'] = {'n_neighbors': range(3,12,2), 'weights': ['uniform', 'distance']}\nknn_clf['grid_search'] = 0\nbase_models.append(knn_clf)\n\nlr_clf = {}\nlr_clf['model'] = LogisticRegression(random_state=5)\nlr_clf['params'] = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\nlr_clf['grid_search'] = 0\nbase_models.append(lr_clf)\n\n# Define Stacking estimator\nrf_stk = {}\nrf_stk['model'] = RandomForestClassifier(n_estimators=300, random_state=5)\nrf_stk['grid_search'] = 0\nrf_stk['name'] = rf_stk['model'].__class__.__name__\n\nfor base_model in base_models:\n    base_model['name'] = base_model['model'].__class__.__name__ ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_stacked_model(models, stack_model, X, y, splits):\n    stack = StackingCVClassifier(classifiers=models,\n                             meta_classifier=stack_model,\n                             cv=splits,\n                             stratify=True,\n                             shuffle=True,\n                             use_probas=True,\n                             use_features_in_secondary=True,\n                             verbose=0,\n                             random_state=5,\n                             n_jobs=2)\n\n    stack = stack.fit(X, y)\n    stack_out = copy.deepcopy(stack)\n    return stack_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_combinations = []\nmodel_combinations_score = []\n\n# 3 models\n# Models: [['LGBMClassifier', 'KNeighborsClassifier', 'LogisticRegression']]\n# Score: [0.9014550264550265]\n\n# 7 models\n# Models: [['LGBMClassifier', 'ExtraTreesClassifier', 'AdaBoostClassifier', 'SVC', 'XGBClassifier', 'RandomForestClassifier', 'KNeighborsClassifier']]\n# Score: [0.8991402116402116]\n\n# 6 models\n# Models: [['LGBMClassifier', 'AdaBoostClassifier', 'SVC', 'XGBClassifier', 'RandomForestClassifier', 'KNeighborsClassifier']]\n# Score: [0.8961640211640212]\n    \n# 8 models\n# Models: [['LGBMClassifier', 'ExtraTreesClassifier', 'AdaBoostClassifier', 'SVC', 'XGBClassifier', 'RandomForestClassifier', 'KNeighborsClassifier', 'LogisticRegression']]\n# Score: [0.8912037037037037]\n\n# 4 models\n# Models: [['XGBClassifier', 'RandomForestClassifier', 'KNeighborsClassifier', 'LogisticRegression']]\n# Score: [0.8895502645502645]\n\n# 5 models\n# Models: [['LGBMClassifier', 'AdaBoostClassifier', 'SVC', 'RandomForestClassifier', 'KNeighborsClassifier']]\n# Score: [0.8875661375661376]\n    \n#for r in range(3, 3):#len(base_models)):\ncombs = itertools.combinations(base_models, 8)\nfor comb in combs:\n    model_combinations.append(comb)\n\nif evaluate_model_combinations:\n    for model_combination in model_combinations:\n        models = [copy.deepcopy(model['model']) for model in model_combination]\n        model_names = [model['name'] for model in model_combination]\n        stack_fitted = get_stacked_model(models, stack_model['model'], X_train, y_train, layer_one_folds)\n        pred = stack_fitted.predict(X_val)\n        score = accuracy_score(y_val, pred)\n        model_combination_score = {'models': models, \n                                   'model_names': model_names, \n                                   'score': score}\n        model_combinations_score.append(model_combination_score)\n        #print(model_combinations_score)\n        \n    for model_combinations_score in sorted(model_combinations_score, key=lambda x: x['score'], reverse=True):\n        print('Models: [{models}]\\nScore: [{score}]'\n              .format(models=model_combinations_score['model_names']\n                      , score=model_combinations_score['score']))   \n\nelse:\n    model_combination_score = {'models': [lgbm_clf, knn_clf, lr_clf], \n                               'model_names': ['LGBMClassifier', 'KNeighborsClassifier', 'LogisticRegression'], \n                               'score': 0.9014550264550265}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Identify columns with only 1 value, these are unlikely to be helpful\ncol_singular = [col for col in df.columns if df[col].nunique() == 1]\nprint('Singular columns: {}'.format(col_singular))\n\n# Drop singular columns\ndf.drop(col_singular, axis=1, inplace=True)\ndf_test.drop(col_singular, axis=1, inplace=True)\ndf.drop('Id', axis=1, inplace=True)\ndf_test.drop('Id', axis=1, inplace=True)\n\nX.drop(col_singular, axis=1, inplace=True)\nX.drop('Id', axis=1, inplace=True)\nX_test.drop(col_singular, axis=1, inplace=True)\nX_test.drop('Id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['EV_DTH'] = (df.Elevation - df.Vertical_Distance_To_Hydrology)\ndf_test['EV_DTH'] = (df_test.Elevation - df_test.Vertical_Distance_To_Hydrology)\nX_test['EV_DTH'] = (X_test.Elevation - X_test.Vertical_Distance_To_Hydrology)\nX['EV_DTH'] = (X.Elevation - X.Vertical_Distance_To_Hydrology)\n\ndf['Dis_To_Hy'] = (((df.Horizontal_Distance_To_Hydrology **2) + (df.Vertical_Distance_To_Hydrology **2))**0.5)\ndf_test['Dis_To_Hy'] = (((df_test.Horizontal_Distance_To_Hydrology **2) + (df_test.Vertical_Distance_To_Hydrology **2))**0.5)\nX_test['Dis_To_Hy'] = (((X_test.Horizontal_Distance_To_Hydrology **2) + (X_test.Vertical_Distance_To_Hydrology **2))**0.5)\nX['Dis_To_Hy'] = (((X.Horizontal_Distance_To_Hydrology **2) + (X.Vertical_Distance_To_Hydrology **2))**0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check if target types are evenly spread\nplt.ylabel('frequency')\nplt.xlabel('cover type')\nplt.bar(df['Cover_Type'].unique(), df['Cover_Type'].value_counts(), color ='green', width=0.2)\nplt.rcParams[\"figure.figsize\"] = (5,5)\nplt.show()\n\n# Evenly distributed, that's great**","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Identify important features <a id=\"identify-important-features\"></a>\n[Go back to top](#0)"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Print numerical values of important features\nif get_feature_importances:\n    target = 'Cover_Type'\n    features = list(df.columns)\n    features.remove(target)\n\n    X = df[features]\n    y = df[target]\n\n    bestfeatures = SelectKBest(k=10)\n    fit = bestfeatures.fit(X, y)\n    \n    dfscores = pd.DataFrame(fit.scores_)\n    dfcolumns = pd.DataFrame(X.columns)\n    \n    # Concat two dataframes for better visualization \n    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n    featureScores.columns = ['Specs','Score'] \n    print(featureScores.nlargest(20,'Score'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot graph of feature importances for better visualization\nif get_feature_importances:\n    model = ExtraTreesClassifier()\n    model.fit(X,y)\n    print(model.feature_importances_) \n    \n    feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n    feat_importances.nlargest(10).plot(kind='barh')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Generate heatmap\nif get_feature_importances:\n    # Only considering non-categorical columns for simplicity\n    df_subset = df[['Elevation', 'Aspect', 'Slope',\n           'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n           'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n           'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points',\n           'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n           'Wilderness_Area4', 'Cover_Type']]\n\n    corrmat = df_subset.corr()\n    top_corr_features = corrmat.index\n    plt.figure(figsize=(10,10))\n    g=sns.heatmap(df_subset[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering <a id=\"feature-engineering\"></a>\n[Go back to top](#0)"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Drop low correlation feature\nif drop_low_correlation_features:\n    df.drop(low_correlation_features, axis=1, inplace=True)\n    df_test.drop(low_correlation_features, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get initial scores <a id=\"get-initial-scores\"></a>\n[Go back to top](#0)"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Evaluate Base estimators separately\nif get_initial_scores:\n    for base_model in model_combination_score['models']:\n        model = copy.deepcopy(base_model['model'])\n        # Fit model\n        model.fit(X_train, y_train)\n\n        # Predict\n        y_pred = model.predict(X_val)\n\n        # Calculate accuracy\n        acc = accuracy_score(y_val, y_pred)\n        print('{} Accuracy: {:.2f}%'.format(model.__class__.__name__, acc * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Grid search <a id=\"grid-search\"></a>\n[Go back to top](#0)"},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_optimum_parameters(input_model, params: dict, splits: int, X_in: pd.DataFrame, y_in: pd.Series):\n    model = copy.deepcopy(input_model)\n    print('Model: {model_name}'.format(model_name=input_model.__class__.__name__))\n    print('Optimizing parameters: [{params}]'.format(params=params))\n    kfold = KFold(n_splits=splits, shuffle=True)\n    CV = GridSearchCV(model\n                      , param_grid=base_model['parameters']\n                      , scoring = 'accuracy'\n                      , n_jobs=-1\n                      , cv=kfold)\n    CV.fit(X_in, y_in)\n    output_model = CV.best_estimator_\n    return output_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Do grid search on each base model\nif grid_search:\n    for base_model in model_combination_score['models']:\n        if base_model['grid_search']:\n            base_model['best_model'] = get_optimum_parameters(base_model['model']\n                                                              , base_model['params']\n                                                              , grid_search_n_splits\n                                                              , X_train\n                                                              , y_train)\n\nfor base_model in model_combination_score['models']:\n    if 'best_model' not in base_model:\n        base_model['best_model'] = base_model['model']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"if grid_search:\n    for base_model in model_combination_score['models']:\n        if base_model['grid_search']:\n            model = copy.deepcopy(base_model['best_model'])\n            print('After grid search: ')\n            y_pred = model.predict(X_val)\n            acc = accuracy_score(y_val, y_pred)\n            print('{} Accuracy: {:.2f}%\\n'.format(model.__class__.__name__, acc * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"models = [model_dict['model'] for model_dict in model_combination_score['models']]\nstack = get_stacked_model(models, rf_stk['model'], X, y, layer_one_folds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get final parameters <a id=\"get-final-parameters\"></a>\n[Go back to top](#0)"},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Final params\\n')\nfor base_model in model_combination_score['models']:\n    final_model = base_model['best_model']\n    model_name = final_model.__class__.__name__\n    model_params = final_model.get_params()\n    print('Base model: [{model}]'.format(model=model_name))\n    print('Model params: {params}'.format(params=json.dumps(model_params, indent = 4)))\n\nfinal_stack_model = rf_stk['model']\nmodel_name = final_stack_model.__class__.__name__\nmodel_params = final_stack_model.get_params()\nprint('Stack model: [{model}]'.format(model=model_name))\nprint('Model params: {params}'.format(params=json.dumps(model_params, indent = 4)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate output <a id=\"generate-output\"></a>\n[Go back to top](#0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final output\nif generate_output:\n    preds = stack.predict(df_test)\n    pred_test = stack.predict(X_test)\n    \n    # Save test predictions to file\n    output = pd.DataFrame({'Id': df_sample_submission.Id,\n                   'Cover_Type': preds})\n    output.head()\n    output.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}