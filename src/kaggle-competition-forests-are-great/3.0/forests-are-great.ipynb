{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Intro\n\nhttps://www.kaggle.com/c/learn-together\n\nMy attempt at the contest, upvoted kernels, which I found useful.\n\n**Resources**\n\n* [How to add table of contents](https://www.kaggle.com/questions-and-answers/69732)\n* [Hyperparameter Tuning](https://towardsdatascience.com/hyperparameter-tuning-c5619e7e6624)\n* [Stacking: Improve model performance](https://dkopczyk.quantee.co.uk/stacking/)"},{"metadata":{},"cell_type":"markdown","source":"# Versioning\n\n* Version: 3.0\n* Steps: \n  - Feature importance visualization\n  - Manual stacking, with layer one models below and RandomForest stacking model\n    - RandomForestClassifier\n    - XGBClassifier\n    - AdaBoostClassifier\n    - SVC\n    - KNeighborsClassifier\n  - Hyperparameter optimization for layer one using grid search\n  - For identifying stack model and validation, using X_train, y_train; For df_test fitting on X, Y"},{"metadata":{},"cell_type":"markdown","source":"# Table of contents <a id=\"0\"></a>\n* [Imports](#imports)\n* [Block selection](#block-selection)\n* [Identify important Features](#identify-important-features)\n* [Feature engineering](#feature-engineering)\n* [Separate features and target](#separate-features-and-target)\n* [Define models](#define-models)\n* [Get initial scores](#get-initial-scores)\n* [Grid search](#grid-search)\n* [Generate output](#generate-output)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\ndir = !ls -a\nif ('kernel-metadata.json' in dir):\n    src = 'Local'\n    # Local environment\n    data_path = './data/learn-together'\nelse:\n    # Kaggle environment\n    src = 'Kaggle'\n    data_path = '../input/learn-together'\n\nprint('Environment set to [{env}]'.format(env=src))\nfor dirname, _, filenames in os.walk(data_path):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imports <a id=\"imports\"></a>\n[Go back to top](#0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Suppress future defaults warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# System imports\nimport copy\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Models\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\n# Utilities\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Block selection <a id=\"block-selection\"></a>\n[Go back to top](#0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_feature_importances = 1\ndrop_low_correlation_features = 1\ngrid_search = 1\nvalidation = 0\ngenerate_output = 1\n\nlow_correlation_features = []\ngrid_search_n_splits = 5\nlayer_one_folds = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(data_path + '/test.csv')\ndf_sample_submission = pd.read_csv(data_path + '/sample_submission.csv')\ndf = pd.read_csv(data_path + '/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identify columns with only 1 value, these are unlikely to be helpful\ncol_singular = [col for col in df.columns if df[col].nunique() == 1]\nprint('Singular columns: {}'.format(col_singular))\n\n# Drop singular columns\ndf.drop(col_singular, axis=1, inplace=True)\ndf_test.drop(col_singular, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if target types are evenly spread\nplt.ylabel('frequency')\nplt.xlabel('cover type')\nplt.bar(df['Cover_Type'].unique(), df['Cover_Type'].value_counts(), color ='green', width=0.2)\nplt.rcParams[\"figure.figsize\"] = (5,5)\nplt.show()\n\n# Evenly distributed, that's great**","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Identify important features <a id=\"identify-important-features\"></a>\n[Go back to top](#0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print numerical values of important features\nif get_feature_importances:\n    target = 'Cover_Type'\n    features = list(df.columns)\n    features.remove(target)\n\n    X = df[features]\n    y = df[target]\n\n    bestfeatures = SelectKBest(k=10)\n    fit = bestfeatures.fit(X, y)\n    \n    dfscores = pd.DataFrame(fit.scores_)\n    dfcolumns = pd.DataFrame(X.columns)\n    \n    # Concat two dataframes for better visualization \n    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n    featureScores.columns = ['Specs','Score'] \n    print(featureScores.nlargest(20,'Score'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot graph of feature importances for better visualization\nif get_feature_importances:\n    model = ExtraTreesClassifier()\n    model.fit(X,y)\n    print(model.feature_importances_) \n    \n    feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n    feat_importances.nlargest(10).plot(kind='barh')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate heatmap\nif get_feature_importances:\n    # Only considering non-categorical columns for simplicity\n    df_subset = df[['Elevation', 'Aspect', 'Slope',\n           'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n           'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n           'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points',\n           'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n           'Wilderness_Area4', 'Cover_Type']]\n\n    corrmat = df_subset.corr()\n    top_corr_features = corrmat.index\n    plt.figure(figsize=(10,10))\n    g=sns.heatmap(df_subset[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering <a id=\"feature-engineering\"></a>\n[Go back to top](#0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop low correlation feature\nif drop_low_correlation_features:\n    df.drop(low_correlation_features, axis=1, inplace=True)\n    df_test.drop(low_correlation_features, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Separate features and target <a id=\"separate-features-and-target\"></a>\n[Go back to top](#0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate features and target\ntarget = 'Cover_Type'\nfeatures = list(df.columns)\nfeatures.remove(target)\n\nX = df[features]\ny = df[target]\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, train_size=0.8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define models <a id=\"define-models\"></a>\n[Go back to top](#0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define Base (level 0) and Stacking (level 1) estimators\n# Commented rows were during Hyperparameter optimization run\nbase_models = []\n\nmodel = {'model': LGBMClassifier(n_estimators=300,\n                        num_leaves=128,\n                        verbose=-1,\n                        random_state=5,\n                        n_jobs=1)}\nmodel['grid_search'] = 0\nbase_models.append(model)\n\nmodel = {'model': ExtraTreesClassifier(n_estimators=300,\n                              min_samples_leaf=2,\n                              min_samples_split=2,\n                              max_depth=50,\n                              random_state=5,\n                              n_jobs=1)}\nmodel['grid_search'] = 0\nbase_models.append(model)\n         \nmodel = {'model': AdaBoostClassifier(n_estimators=200, random_state=5)}\n#model = {'model': AdaBoostClassifier(algorithm=\"SAMME.R\"\n#                                     , learning_rate=1.0\n#                                     , n_estimators=200\n#                                     , random_state=5)}\nparameters = {'n_estimators': [100, 150, 200, 400]}\nmodel['parameters'] = parameters\nmodel['grid_search'] = 0\nbase_models.append(model)\n\nmodel = {'model': SVC(probability=True, random_state=5, gamma='scale')}\n#model = {'model': SVC(probability=True\n#                      , C=10\n#                      , random_state=5)}\nCs = [0.01, 0.1, 1, 10, 100]\n#gammas = [0.001, 0.01, 0.1, 1]\nparameters = {'C': Cs}#, 'gamma' : gammas}\nmodel['parameters'] = parameters\nmodel['grid_search'] = 0\nbase_models.append(model)\n\nmodel = {'model': XGBClassifier(random_state=5)}\nparameters = {}\nmodel['parameters'] = parameters\nmodel['grid_search'] = 0\nbase_models.append(model)\n\nmodel = {'model': RandomForestClassifier(n_estimators=300, random_state = 5)}\n#model = {'model': RandomForestClassifier(n_estimators=800, random_state = 5)}\nparameters = {'n_estimators': [100, 150, 200, 400, 600, 800, 1000, 1100]}\nmodel['parameters'] = parameters\nmodel['grid_search'] = 0\nbase_models.append(model)\n\nmodel = {'model': KNeighborsClassifier()}\n#model = {'model': KNeighborsClassifier(n_neighbors=3\n#                                      , weights='distance')}\nparameters = {'n_neighbors': range(3,12,2), \n              'weights': ['uniform', 'distance']}\nmodel['parameters'] = parameters\nmodel['grid_search'] = 1\n#base_models.append(model)\n\nmodel = {'model': LogisticRegression(random_state=5)}\nparameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\nmodel['parameters'] = parameters\nmodel['grid_search'] = 1\n#base_models.append(model)\n\n# Define Stacking estimator\nstack_model = RandomForestClassifier(n_estimators=300, random_state=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get initial scores <a id=\"get-initial-scores\"></a>\n[Go back to top](#0)"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Evaluate Base estimators separately\nfor base_model in base_models:\n    model = copy.deepcopy(base_model['model'])\n    # Fit model\n    model.fit(X_train, y_train)\n    \n    # Predict\n    y_pred = model.predict(X_val)\n    \n    # Calculate accuracy\n    acc = accuracy_score(y_val, y_pred)\n    print('{} Accuracy: {:.2f}%'.format(model.__class__.__name__, acc * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Grid search <a id=\"grid-search\"></a>\n[Go back to top](#0)"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Do grid search on each base model\nif grid_search:\n    for base_model in base_models:\n        if base_model['grid_search']:\n            print('Model: {model_name}'.format(model_name=base_model['model'].__class__.__name__))\n            print('Optimizing parameters: [{params}]'.format(params=base_model['parameters']))\n            kfold = KFold(n_splits=grid_search_n_splits, shuffle=True)\n            CV = GridSearchCV(base_model['model']\n                          , param_grid=base_model['parameters']\n                          , scoring = 'accuracy'\n                          , n_jobs=-1\n                          , cv=kfold)\n            CV.fit(X_train, y_train)\n            best_model = CV.best_estimator_\n            base_model['best_model'] = best_model\n            #print('Best score and parameter combination = ')\n            #print(CV.best_score_)    \n            #print(CV.best_params_) \n            #print('\\n')\n\nfor base_model in base_models:\n    if 'best_model'not in base_model:\n        base_model['best_model'] = base_model['model']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"if grid_search:\n    for base_model in base_models:\n        if base_model['grid_search']:\n            model = copy.deepcopy(base_model['best_model'])\n            print('After grid search: ')\n            y_pred = model.predict(X_val)\n            acc = accuracy_score(y_val, y_pred)\n            print('{} Accuracy: {:.2f}%\\n'.format(model.__class__.__name__, acc * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stacking intro <a id=\"stacking-intro\"></a>\n[Go back to top](#0)"},{"metadata":{},"cell_type":"markdown","source":"## Layer 1\n1. Layer 1, loop across folds\n  - Separate X and y into training and validation sets. Keep validation set aside and ignore for now\n  - For the first base model, split the training data into n folds\n  - Fit the base model on X_train, y_train from n-1 folds, and predict on the remaining nth fold. \n  - Add the predictions into a meta series for the base model\n2. Repeat (1), n times, each time with new nth fold. This will cover full training data. Will then end up with predictions for all n folds (full training data) in the meta seies (number of rows same as X_train)\n3. Loop across models\n  - Repeat (1) and (2) for all base models\n  - Combine meta series for each base model into meta df - one column per base model, number of rows same as original training data. \n4. Optionally add an original feature from the training data into the meta dataframe\n5. This dataframe is feature data for next stage\n\n## Layer 2\n6. Fit the stacking model on the meta Dataframe (output of (6)) and y_train. This is our stacked model.\n\n## Validation\n7. Fit first base model on the full input data (X, y). Predict on X_val, this will generate a meta series\n8. Repeat (8) across each base model, combine output series from each base model to make meta dataframe for next layer\n9. Add the same feature column (as in (5)), from X_val into the meta dataframe from (9). This is source for next layer\n10. Predict using stacked model (output of (7)) on the output of (10)\n11. Compare prediction from (11) with y_val to get score\n\n## Testing\n12. Repeat steps (7), (8), (9), (10), this time using df_test in place of X_val"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create first level predictions (meta-features)\ndef hold_out_predict(clf, X, y, cv):\n        \n    \"\"\"Performing cross validation hold out predictions for stacking\"\"\"\n    # Initilize\n    n_classes = len(np.unique(y)) # Assuming that training data contains all classes\n    meta_features = np.zeros((X.shape[0], n_classes)) \n    n_splits = cv.get_n_splits(X, y)\n    \n    # Loop over folds\n    print(\"Starting hold out prediction with {} splits for {}.\".format(n_splits, clf.__class__.__name__))\n    for train_idx, hold_out_idx in cv.split(X): \n        \n        # Split data\n        X_train = X.iloc[train_idx]    \n        y_train = y.iloc[train_idx]\n        X_hold_out = X.iloc[hold_out_idx]\n\n        # Fit estimator to K-1 parts and predict on hold out part\n        est = copy.deepcopy(clf)\n        est.fit(X_train, y_train)\n        y_hold_out_pred = est.predict_proba(X_hold_out)\n        \n        # Fill in meta features\n        meta_features[hold_out_idx] = y_hold_out_pred\n\n    return meta_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create first level predictions (meta-features) from training data\n\n# Define folds\nkfold = KFold(n_splits=layer_one_folds, shuffle=True)\n\n# Loop over classifier to produce meta features\nmeta_train = pd.DataFrame()\nfor base_model in base_models:\n    \n    model = copy.deepcopy(base_model['best_model'])\n    # Create hold out predictions for a classifier\n    meta_train_model = hold_out_predict(model, X_train, y_train, kfold)\n    #print(pd.DataFrame(meta_train_model).head())\n    \n    # Gather meta training data\n    meta_train = pd.concat([meta_train, pd.DataFrame(meta_train_model)], axis=1)\n    #print(pd.DataFrame(meta_train).head())\n\nstack_model.fit(meta_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation <a id=\"validation\"></a>\n[Go back to top](#0)"},{"metadata":{"trusted":false},"cell_type":"code","source":"if validation:\n    # Create meta-features for testing data\n    meta_val = pd.DataFrame()\n    for base_model in base_models:\n\n        model = copy.deepcopy(base_model['best_model'])\n        # Create hold out predictions for a classifier\n        model.fit(X, y)\n        meta_val_model = model.predict_proba(X_val)\n\n        meta_val = pd.concat([meta_val, pd.DataFrame(meta_val_model)], axis=1)\n\n    y_pred = stack_model.predict(meta_val)\n    score = accuracy_score(y_val, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get final parameters <a id=\"get-final-parameters\"></a>\n[Go back to top](#0)"},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Final params\\n')\nfor base_model in base_models:\n    final_model = base_model['best_model']\n    model_name = final_model.__class__.__name__\n    model_params = final_model.get_params()\n    print('Base model: [{model}]'.format(model=model_name))\n    print('Model params: {params}'.format(params=json.dumps(model_params, indent = 4)))\n\nfinal_stack_model = stack_model\nmodel_name = final_stack_model.__class__.__name__\nmodel_params = final_stack_model.get_params()\nprint('Stack model: [{model}]'.format(model=model_name))\nprint('Model params: {params}'.format(params=json.dumps(model_params, indent = 4)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate output <a id=\"generate-output\"></a>\n[Go back to top](#0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final output\nif generate_output:\n    meta_test = pd.DataFrame()\n    for base_model in base_models:\n        model = copy.deepcopy(base_model['best_model'])\n        \n        # Fit model\n        model.fit(X, y)\n        meta_test_model = model.predict_proba(df_test)\n    \n        # Gather meta training data\n        meta_test = pd.concat([meta_test, pd.DataFrame(meta_test_model)], axis=1)\n    \n    # Final output\n    preds = stack_model.predict(meta_test)\n\n    # Save test predictions to file\n    output = pd.DataFrame({'Id': df_sample_submission.Id,\n                   'Cover_Type': preds})\n    output.head()\n    output.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}