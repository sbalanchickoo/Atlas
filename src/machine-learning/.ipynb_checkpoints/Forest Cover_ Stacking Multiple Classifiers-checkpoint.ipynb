{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "random_state = 1\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "\n",
    "\n",
    "print('> Loading data')\n",
    "X_train = pd.read_csv('/kaggle/input/learn-together/train.csv', index_col='Id')\n",
    "X_test = pd.read_csv('/kaggle/input/learn-together/test.csv', index_col='Id')\n",
    "\n",
    "y_train = X_train['Cover_Type']\n",
    "X_train = X_train.drop(['Cover_Type'], axis='columns')\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/kwabenantim/forest-cover-feature-engineering\n",
    "def add_features(X_):\n",
    "    X = X_.copy()\n",
    "\n",
    "    X['Hydro_Elevation_diff'] = (X['Elevation'] - \n",
    "                                 X['Vertical_Distance_To_Hydrology'])\n",
    "\n",
    "    X['Hydro_Euclidean'] = (X['Horizontal_Distance_To_Hydrology']**2 +\n",
    "                            X['Vertical_Distance_To_Hydrology']**2).apply(np.sqrt)\n",
    "\n",
    "    X['Hydro_Fire_sum'] = (X['Horizontal_Distance_To_Hydrology'] + \n",
    "                           X['Horizontal_Distance_To_Fire_Points'])\n",
    "\n",
    "    X['Hydro_Fire_diff'] = (X['Horizontal_Distance_To_Hydrology'] - \n",
    "                            X['Horizontal_Distance_To_Fire_Points']).abs()\n",
    "\n",
    "    X['Hydro_Road_sum'] = (X['Horizontal_Distance_To_Hydrology'] +\n",
    "                           X['Horizontal_Distance_To_Roadways'])\n",
    "\n",
    "    X['Hydro_Road_diff'] = (X['Horizontal_Distance_To_Hydrology'] -\n",
    "                            X['Horizontal_Distance_To_Roadways']).abs()\n",
    "\n",
    "    X['Road_Fire_sum'] = (X['Horizontal_Distance_To_Roadways'] + \n",
    "                          X['Horizontal_Distance_To_Fire_Points'])\n",
    "\n",
    "    X['Road_Fire_diff'] = (X['Horizontal_Distance_To_Roadways'] - \n",
    "                           X['Horizontal_Distance_To_Fire_Points']).abs()\n",
    "    \n",
    "    # For all 40 Soil_Types, 1=rubbly, 2=stony, 3=very stony, 4=extremely stony, 0=?\n",
    "    stoneyness = [4, 3, 1, 1, 1, 2, 0, 0, 3, 1, \n",
    "                  1, 2, 1, 0, 0, 0, 0, 3, 0, 0, \n",
    "                  0, 4, 0, 4, 4, 3, 4, 4, 4, 4, \n",
    "                  4, 4, 4, 4, 1, 4, 4, 4, 4, 4]\n",
    "    \n",
    "    # Compute Soil_Type number from Soil_Type binary columns\n",
    "    X['Stoneyness'] = sum(i * X['Soil_Type{}'.format(i)] for i in range(1, 41))\n",
    "    \n",
    "    # Replace Soil_Type number with \"stoneyness\" value\n",
    "    X['Stoneyness'] = X['Stoneyness'].replace(range(1, 41), stoneyness)  \n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def drop_features(X_):\n",
    "    X = X_.copy()\n",
    "    \n",
    "    # Drop low variance columns\n",
    "    hi = 0.99 * len(X)\n",
    "    lo_var = [c for c in X.columns if  X[c].value_counts().iat[0] > hi]\n",
    "    \n",
    "    X = X.drop(lo_var, axis='columns')\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "print('> Adding features')\n",
    "X_train = add_features(X_train)\n",
    "X_test = add_features(X_test)\n",
    "\n",
    "\n",
    "print('> Setting up classifiers')\n",
    "ab_clf = AdaBoostClassifier(n_estimators=200,\n",
    "                            base_estimator=DecisionTreeClassifier(\n",
    "                                min_samples_leaf=2,\n",
    "                                random_state=random_state),\n",
    "                            random_state=random_state)\n",
    "\n",
    "et_clf = ExtraTreesClassifier(n_estimators=300,\n",
    "                              min_samples_leaf=2,\n",
    "                              min_samples_split=2,\n",
    "                              max_features=30,\n",
    "                              max_depth=50,\n",
    "                              random_state=random_state)\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=200,\n",
    "                                random_state=random_state)\n",
    "\n",
    "\n",
    "predictions = pd.Series(dtype=y_train.dtype)\n",
    "wilderness_areas = ['Wilderness_Area{}'.format(i) for i in range(1,5)]\n",
    "\n",
    "for wa in range(1,5):\n",
    "    print('> Preparing data for Wilderness_Area{}'.format(wa))\n",
    "    \n",
    "    X_train_wa = X_train[X_train['Wilderness_Area{}'.format(wa)] == 1]\n",
    "    X_test_wa = X_test[X_test['Wilderness_Area{}'.format(wa)] == 1]\n",
    "    \n",
    "    X_train_wa = X_train_wa.drop(wilderness_areas, axis='columns')\n",
    "    X_test_wa = X_test_wa.drop(wilderness_areas, axis='columns')\n",
    "    \n",
    "    y_train_wa = y_train.loc[X_train_wa.index]\n",
    "    \n",
    "    \n",
    "    print('> Adding soil count features')\n",
    "    X_train_wa['Soil_counts'] = sum(i * X_train_wa['Soil_Type{}'.format(i)] for i in range(1, 41))\n",
    "    X_test_wa['Soil_counts'] = sum(i * X_test_wa['Soil_Type{}'.format(i)] for i in range(1, 41))\n",
    "\n",
    "    soils = pd.concat([X_train_wa['Soil_counts'], X_test_wa['Soil_counts']], ignore_index=True)\n",
    "    soil_counts = soils.value_counts()\n",
    "\n",
    "    X_train_wa['Soil_counts'] = X_train_wa['Soil_counts'].map(soil_counts)\n",
    "    X_test_wa['Soil_counts'] = X_test_wa['Soil_counts'].map(soil_counts)\n",
    "\n",
    "    \n",
    "    print('> Dropping features')\n",
    "    X_train_wa = drop_features(X_train_wa)\n",
    "    X_test_wa = X_test_wa[X_train_wa.columns]\n",
    "    print('> {} features remaining'.format(len(X_train_wa.columns)))\n",
    "    \n",
    "    \n",
    "    print('> Up-sampling')\n",
    "    max_samples = y_train_wa.value_counts().iat[0]\n",
    "    classes = y_train_wa.unique().tolist()\n",
    "    sampling_strategy = dict((cl, max_samples) for cl in classes)\n",
    "    \n",
    "    sampler = SMOTE(sampling_strategy=sampling_strategy,\n",
    "                    random_state=random_state)\n",
    "    \n",
    "    X_train_wa, y_train_wa = sampler.fit_resample(X_train_wa, y_train_wa)\n",
    "    X_train_wa = pd.DataFrame(X_train_wa)\n",
    "    y_train_wa = pd.Series(y_train_wa)\n",
    "    \n",
    "    \n",
    "    print('> Setting up stack')   \n",
    "    max_features = min(30, X_train_wa.columns.size)\n",
    "    et_clf.set_params(max_features=max_features)\n",
    "    \n",
    "    stack = StackingCVClassifier(classifiers=[ab_clf, et_clf, rf_clf],\n",
    "                                 meta_classifier=rf_clf,\n",
    "                                 cv=5,\n",
    "                                 stratify=True,\n",
    "                                 shuffle=True,\n",
    "                                 use_probas=True,\n",
    "                                 use_features_in_secondary=True,\n",
    "                                 verbose=1,\n",
    "                                 random_state=random_state,\n",
    "                                 n_jobs=-1\n",
    "                                )\n",
    "    \n",
    "    \n",
    "    print('> Fitting stack for Wilderness_Area{}'.format(wa))      \n",
    "    stack = stack.fit(X_train_wa, y_train_wa)\n",
    "    \n",
    "    \n",
    "    print('> Making predictions for Wilderness_Area{}'.format(wa))\n",
    "    prediction_wa = stack.predict(X_test_wa)\n",
    "    prediction_wa = pd.DataFrame(prediction_wa, index=X_test_wa.index)\n",
    "    \n",
    "    predictions = pd.concat([predictions, prediction_wa])\n",
    "    \n",
    "\n",
    "print('> Creating submission')\n",
    "predictions = predictions.sort_index()\n",
    "predictions.to_csv('submission.csv', header=['Cover_Type'], index=True, index_label='Id')\n",
    "\n",
    "print('> Done !')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
